#+title: Homework 3:  Machine Learning for Language Understanding
#+author: Toni Kazic
#+date: Fall, 2024

# <2023-10-24 Tue>


#+SETUPFILE: "../../../common/preamble.org"
#+LATEX_CLASS: article
#+OPTIONS: toc:nil
#+OPTIONS: ^:nil

#+LATEX_HEADER: \usepackage{langsci-avm}
# http://ftp.math.purdue.edu/mirrors/ctan.org/macros/latex/contrib/langsci-avm/langsci-avm.pdf

#+LATEX_HEADER: \newcommand{\grmr}[2]{\ensuremath{\mathrm{#1} & \,\longrightarrow\, \mathrm{#2}}}
#+LATEX_HEADER: \newcommand{\txtgrmr}[2]{\ensuremath{\mathrm{#1} \,\longrightarrow\, \mathrm{#2}}}
#+LATEX_HEADER: \newcommand{\grmrhs}[1]{\ensuremath{& \,\longrightarrow\, \mathrm{#1} }}
#+LATEX_HEADER: \newcommand{\wa}[1]{\type{\textnormal{\w{#1}}}}

# compile with pdflatex
#
# Kazic, 3.11.2020



* DONE Introduction

This homework explores the course objective of understanding and
generating machine learning approaches, taking us into (shallow) machine
learning for language understanding. This will give us a chance to consider
the problem of usefully embedding language in numerically defined spaces,
giving us a chance to compare a seat-of-the-pants approach to the more
sophisticated ones we discussed in class.  It also gives us an opportunity
to compare different classifiers for different corpora and get an idea of
how well they perform.


NLTK has some classifiers, but [[http://scikit-learn.org/stable/user_guide.html][scikit-learn]] has more (so be sure to install
any needed modules!).  Let's jump off the deep end together and see how
far we can get.



# We'll revisit the feature extraction problem in the future, so
# this homework explores how well guessed features perform.

# At the end we look briefly at checking coherence using bags of
# words (see [[http://www.nltk.org/book/ch06.html][Section 2.3 ff]] for a short discussion and examples).  This is
# one technique for recognizing textual entailment.




* DONE Who's Who and Solution Patterns
<<whoswho>>

** Lead Person:  green


** Group Members

| first name last name | color                                |
|----------------------+--------------------------------------|
| Huzaifa Khan         | green \color{green}\rule{5mm}{3mm}   |
| Michael Huber        | yellow \color{yellow}\rule{5mm}{3mm} |
| Ethan Glass          | purple \color{violet}\rule{5mm}{3mm} |



** *Special Instructions for this Homework!*

All group members are to work on problem 6 together.  I want you to have
enough scope to play around with the embeddings and clustering algorithms.



** Three Member Solution Patterns

$i$ is the question number.

#+begin_center
#+ATTR_LaTeX: :mode inline-math :environment array
| \text{color}                  | \text{draft solution} | \text{revise solution} |
|-------------------------------+----------------+-----------------|
| green \color{green}\rule{5mm}{3mm} | i \mod 3 = 1   | i \mod 3 = 0    |
| yellow \color{yellow}\rule{5mm}{3mm} | i \mod 3 = 2   | i \mod 3 = 1    |
| purple \color{violet}\rule{5mm}{3mm} | i \mod 3 = 0   | i \mod 3 = 2    |
#+end_center

** Two Member Solution Patterns

| color                         | draft solution | revise solution |
|-------------------------------+----------------+-----------------|
| green \color{green}\rule{5mm}{3mm} | odds           | evens           |
| yellow \color{yellow}\rule{5mm}{3mm} | evens          | odds            |




* DONE General Instructions
  + /Fill out the group members table and follow the solution patterns/ in
     Section [[whoswho]].

   + /If the question is unclear, tell me your interpretation of it as part
     of your answer./  Feel free to ask about the questions in class or on
     the Slack channel (use =@channel= as others will probably be puzzled
     too). 

   + /For questions using corpora, use the corpus of the lead person./

   + /Put your draft answers right after each question using a *complete,
     functional* =org= mode code or example block./ Make sure your code
     block is complete and functional by testing it in your copy of this
     homework file.

   + /Each group member reviews the others' draft solutions and you revise them together/.

   + /Discuss each other's draft and reviews, finalizing the answers./

   + /Show all your work: code, results, and analysis./  Does your code
     work in this file and produce *exactly* the results you show? 

   + /Post the completed file to Canvas no later than noon on the Tuesday
     indicated/ in the [[../syllabus.org::schedule][schedule in the syllabus]], naming your file with each
     person's first name (no spaces in the file name, and don't forget the
     =.org= extension!).  Only one person should submit the final file.


* DONE Hints


** joblib.dump() and joblib.load()

Note that for big data, the [[http://scikit-learn.org/stable/tutorial/basic/tutorial.html][scikit-learn tutorial]] suggests using
joblib.dump() and joblib.load().  However this only writes to disk, not a
string, but can store models and classifiers.


** I expect you to bring questions and struggles with problems 4 and 5 to class!


** You may need to reformat the data generated by NLTK for the [[http://scikit-learn.org/stable/modules/feature_extraction.html][scikit-learn code]].


** You may need to cope with [[http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py][sparse data]] in scikit-learn.

One reason to use the universal tagset is that it doesn't have very many
tags, but depending on your corpus, your suffixes may have more than is
really desirable for problem 4.




* DONE Questions

# revised for exclusions for 2024
#
# Kazic, 12.12.2023

1. [@1] <<tagging>> Randomly select 1000 POS-tagged words from your
   assigned category/corpus and form your training and testing sets so that
   you train on 90% of the words and test on 10%.  Exclude stop words,
   numbers, and words of length three or less. Use the universal POS
   tagset. *If your corpus isn't tagged, choose a tagged corpus.* Using a
   tagger to tag it won't help much as it doesn't make a gold standard
   (that requires hand-checking by expert grammarians).  Save these somehow
   so you can re-use them in the [[fea-set][next]] problem.

#+BEGIN_SRC python :results output
import pandas as pd
import nltk
from nltk.corpus import gutenberg, stopwords
from nltk import pos_tag
from nltk.data import load

import random
from sklearn.model_selection import train_test_split

# UNCOMMENT THESE TO DOWNLOAD NLTK CONTENT AS NECESSARY -----------------------------------------
# nltk.download('gutenberg')
# nltk.download('stopwords')
# nltk.download('averaged_perceptron_tagger_eng')
# nltk.download('universal_tagset')

# gets the set of stopwords
sws = set(stopwords.words("english")) 

set_of_words = set() # stores unique words

for fileid in gutenberg.fileids():
    for word in gutenberg.words(fileid):
        if word.isalpha() and len(word) > 3 and word.lower() not in sws:
            set_of_words.add(word.lower())

# print(set_of_words)

oneK_words = random.sample(list(set_of_words), 1000)

tagged_words = pos_tag(oneK_words, tagset='universal')

train_set, test_set = train_test_split(tagged_words, test_size=0.1, random_state=42)

print(f"The test set has {len(test_set)} entries, and the training set has {len(train_set)} entries")

train_df = pd.DataFrame(train_set, columns=['Word', 'POS'])
test_df = pd.DataFrame(test_set, columns=['Word', 'POS'])
train_df.to_csv("hw3_train_set.csv", index=False)
test_df.to_csv("hw3_test_set.csv", index=False)

print("train and test set stored as csv")
  
#+END_SRC

#+RESULTS:
: The test set has 100 entries, and the training set has 900 entries
: train and test set stored as csv


2. [@2] <<fea-set>> For each of tuples in your training set in problem
   [[tagging]], extract the last *three /letters/ for the word* and its entire
   tag, and compile these into an NLTK feature set.  Check to see if the
   same feature is mapped to more than one tag and if so, then retag that
   feature with 'X'.  Again, save the feature set for reuse as a file, at
   minimum as a dictionary.  *Do the same for the test set.*

#+BEGIN_SRC python :results output
import pandas as pd

#using pickle as it saves state of data for future
import pickle

# Load the training and test sets
train_df = pd.read_csv("hw3_train_set.csv")
test_df = pd.read_csv("hw3_test_set.csv")

# Helper function to create feature sets with last three letters
def create_feature_set(df):
    feature_dict = {}
    for _, row in df.iterrows():
        word, pos = row['Word'], row['POS']
        last_three = word[-3:]  # Get the last three letters
        
        # Check if last_three already exists and has a different tag
        if last_three in feature_dict and feature_dict[last_three] != pos:
            feature_dict[last_three] = 'X'  # Mark ambiguous feature
        else:
            feature_dict[last_three] = pos
    return feature_dict

# Create feature sets for training and testing
train_features = create_feature_set(train_df)
test_features = create_feature_set(test_df)

# Save feature sets as files
with open("train_features.pkl", "wb") as train_file:
    pickle.dump(train_features, train_file)
    
with open("test_features.pkl", "wb") as test_file:
    pickle.dump(test_features, test_file)

print("Feature sets saved as pickle files.")

#+END_SRC

#+RESULTS:
: Feature sets saved as pickle files.

3. [@3] <<bayesian>> Now train a naive Bayes classifier using the one in
   NLTK or scikit-learn on your saved [[fea-set][feature set]] and show its accuracy.
   Show the ten most informative features (for sure you can get this from
   the NLTK naive Bayes classifier).


#+BEGIN_SRC python :results output
import pickle
import nltk
from nltk.classify import NaiveBayesClassifier
from nltk.metrics import accuracy

with open("train_features.pkl", "rb") as train_file:
    train_features = pickle.load(train_file)

with open("test_features.pkl", "rb") as test_file:
    test_features = pickle.load(test_file)

def extract_features(word):
    features = {
        "last_three": word[-3:],
        "last_two": word[-2:],
        "first_letter": word[0],
        "length": len(word)
    }
    return features

train_set = [(extract_features(word), pos) for word, pos in train_features.items()]
test_set = [(extract_features(word), pos) for word, pos in test_features.items()]

classifier = NaiveBayesClassifier.train(train_set)

accuracy_score = nltk.classify.accuracy(classifier, test_set)
print(f"Naive Bayes Classifier Accuracy: {accuracy_score * 100:.2f}%")

print("\nTop 10 Most Informative Features:")
classifier.show_most_informative_features(10)
#+END_SRC

#+RESULTS:
#+begin_example
Naive Bayes Classifier Accuracy: 43.04%

Top 10 Most Informative Features:
Most Informative Features
                last_two = 'ly'              ADV : ADJ    =      8.9 : 1.0
            first_letter = 'x'               ADV : NOUN   =      6.7 : 1.0
                last_two = 'es'                X : ADJ    =      5.4 : 1.0
            first_letter = 'h'              VERB : X      =      5.0 : 1.0
                last_two = 'en'              ADJ : NOUN   =      4.6 : 1.0
                last_two = 'er'                X : VERB   =      4.6 : 1.0
                last_two = 'ed'                X : ADJ    =      4.5 : 1.0
            first_letter = 'b'               ADV : NOUN   =      4.0 : 1.0
                last_two = 'al'              ADJ : NOUN   =      4.0 : 1.0
                last_two = 'an'                X : NOUN   =      3.9 : 1.0
#+end_example



4. [@4] <<lda>> Continuing with the Bayesian theme of problem [[bayesian]],
   let's try some [[http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis][linear discriminant analysis]] with the algorithm found in
   *scikit-learn*, using the tags in the saved [[fea-set][feature set]] as the labels we
   want to learn.  Again, the question we're asking is how well do the
   three-letter suffixes predict the tags, so this is a multiclass LDA
   problem; the example on the man page shows a two-class problem.  We'll
   use the default =svd= method with no shrinkage, both because it's a good
   place to start and because it may be able to cope with the size of our
   tag and feature sets.  Return the output of the =decision_function= and
   =score= functions after training and testing the classifier.  Compute
   the LDA with and without downcasing and compare and discuss the results.
   
** Based on the results, we get an extremely modest 17% accuracy based on LDA. This result doesn't change based on downcasing, which makes sense as we're only looking at the last last 3 letters of the words, which are most likely all downcased to begin with. See below the code_block for exact accuracy and decision_function outputs.

#+begin_src python :results output
import pickle
import nltk
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import numpy as np
from sklearn.preprocessing import LabelEncoder

with open("train_features.pkl", 'rb') as train_file:
    train_features = pickle.load(train_file)
with open("test_features.pkl", 'rb') as test_file:
    test_features = pickle.load(test_file)

train_X = np.array(list(train_features.keys())).reshape(-1, 1)
train_y = np.array(list(train_features.values()))

test_X = np.array(list(test_features.keys())).reshape(-1, 1)
test_y = np.array(list(test_features.values()))

encoder = LabelEncoder()
train_X = encoder.fit_transform(train_X.ravel()).reshape(-1, 1)
train_y = encoder.fit_transform(train_y)

test_X = encoder.fit_transform(test_X.ravel()).reshape(-1, 1)
test_y = encoder.fit_transform(test_y)


lda = LinearDiscriminantAnalysis()
lda.fit(train_X, train_y)

decision_function_nodowncase = lda.decision_function(test_X)
score = lda.score(test_X, test_y)

print(score)

## DOWNCASE -------------------------------------------------------------------------

# tx = [str(feature).lower() for feature in list(train_features.keys())]
train_X = np.array([str(feature).lower() for feature in list(train_features.keys())]).reshape(-1, 1)
# print(len(train_X))
train_y = np.array(list(train_features.values()))

test_X = np.array([str(feature).lower() for feature in list(test_features.keys())]).reshape(-1, 1)
test_y = np.array(list(test_features.values()))

train_X = encoder.fit_transform(train_X.ravel()).reshape(-1, 1)
test_X = encoder.fit_transform(test_X.ravel()).reshape(-1, 1)

train_y = encoder.fit_transform(train_y)
test_y = encoder.fit_transform(test_y)

lda = LinearDiscriminantAnalysis()
lda.fit(train_X, train_y)

decision_function = lda.decision_function(test_X)
score = lda.score(test_X, test_y)
print(score)
print("Decision Function (NO DOWNCASE)", decision_function_nodowncase)
print("Decision Function (Downcase)", decision_function)
#+end_src

#+RESULTS:
#+begin_example
0.17721518987341772
0.17721518987341772
Decision Function (NO DOWNCASE) [[-1.6440558  -5.64455458 -3.46957271 -5.37883597 -0.85466112 -6.76720441
  -2.07629882 -1.5266892 ]
 [-1.64112034 -5.61713822 -3.46469541 -5.42472695 -0.855474   -6.73904907
  -2.06887941 -1.53620364]
 [-1.65824387 -5.77706694 -3.49314634 -5.15702957 -0.85073218 -6.90328853
  -2.11215931 -1.48070277]
 [-1.65579765 -5.75421998 -3.48908192 -5.19527205 -0.85140958 -6.87982575
  -2.10597646 -1.48863146]
 [-1.66509328 -5.84103842 -3.50452671 -5.04995061 -0.84883545 -6.96898432
  -2.12947127 -1.45850242]
 [-1.64943748 -5.69481788 -3.47851443 -5.29470251 -0.85317083 -6.81882253
  -2.08990107 -1.50924607]
 [-1.66117933 -5.80448329 -3.49802364 -5.11113859 -0.8499193  -6.93144387
  -2.11957872 -1.47118833]
 [-1.66900724 -5.87759356 -3.51102978 -4.98876264 -0.84775161 -7.00652477
  -2.13936381 -1.44581651]
 [-1.6518837  -5.71766484 -3.48257885 -5.25646003 -0.85249343 -6.84228531
  -2.09608391 -1.50131738]
 [-1.64699126 -5.67197093 -3.47445001 -5.33294499 -0.85384823 -6.79535975
  -2.08371823 -1.51717477]
 [-1.64552353 -5.65826275 -3.47201136 -5.35589048 -0.85425468 -6.78128208
  -2.08000852 -1.52193198]
 [-1.65041597 -5.70395667 -3.4801402  -5.27940552 -0.85289987 -6.82820764
  -2.09237421 -1.50607459]
 [-1.64014185 -5.60799944 -3.46306964 -5.44002395 -0.85574496 -6.72966396
  -2.06640627 -1.53937511]
 [-1.66069009 -5.7999139  -3.49721076 -5.11878708 -0.85005478 -6.92675131
  -2.11834215 -1.47277407]
 [-1.67047497 -5.89130173 -3.51346843 -4.96581715 -0.84734516 -7.02060243
  -2.14307352 -1.44105929]
 [-1.65237295 -5.72223424 -3.48339173 -5.24881153 -0.85235795 -6.84697786
  -2.09732048 -1.49973164]
 [-1.64160958 -5.62170762 -3.46550829 -5.41707846 -0.85533852 -6.74374163
  -2.07011597 -1.5346179 ]
 [-1.6675395  -5.86388538 -3.50859113 -5.01170813 -0.84815805 -6.9924471
  -2.13565411 -1.45057373]
 [-1.64503429 -5.65369336 -3.47119848 -5.36353898 -0.85439016 -6.77658952
  -2.07877195 -1.52351772]
 [-1.65726538 -5.76792815 -3.49152057 -5.17232656 -0.85100314 -6.89390342
  -2.10968617 -1.48387425]
 [-1.6562869  -5.75878937 -3.4898948  -5.18762355 -0.8512741  -6.88451831
  -2.10721303 -1.48704573]
 [-1.66460404 -5.83646903 -3.50371383 -5.05759911 -0.84897093 -6.96429176
  -2.1282347  -1.46008816]
 [-1.65922236 -5.78620572 -3.49477211 -5.14173257 -0.85046122 -6.91267365
  -2.11463244 -1.47753129]
 [-1.65775463 -5.77249755 -3.49233346 -5.16467806 -0.85086766 -6.89859598
  -2.11092274 -1.48228851]
 [-1.67096421 -5.89587112 -3.51428132 -4.95816865 -0.84720968 -7.02529499
  -2.14431009 -1.43947355]
 [-1.66607177 -5.85017721 -3.50615248 -5.03465362 -0.84856449 -6.97836943
  -2.1319444  -1.45533094]
 [-1.64894824 -5.69024849 -3.47770155 -5.30235101 -0.85330631 -6.81412997
  -2.0886645  -1.51083181]
 [-1.66949648 -5.88216295 -3.51184266 -4.98111414 -0.84761613 -7.01121732
  -2.14060038 -1.44423077]
 [-1.64748051 -5.67654032 -3.4752629  -5.3252965  -0.85371275 -6.8000523
  -2.0849548  -1.51558903]
 [-1.65873312 -5.78163633 -3.49395922 -5.14938107 -0.8505967  -6.90798109
  -2.11339587 -1.47911703]
 [-1.65677614 -5.76335876 -3.49070769 -5.17997506 -0.85113862 -6.88921087
  -2.1084496  -1.48545999]
 [-1.67487816 -5.93242626 -3.52078439 -4.89698068 -0.84612584 -7.06283544
  -2.15420264 -1.42678764]
 [-1.67585665 -5.94156504 -3.52241015 -4.88168368 -0.84585488 -7.07222055
  -2.15667577 -1.42361616]
 [-1.65432992 -5.7405118  -3.48664327 -5.21821754 -0.85181602 -6.86574809
  -2.10226676 -1.49338868]
 [-1.67536741 -5.93699565 -3.52159727 -4.88933218 -0.84599036 -7.06752799
  -2.15543921 -1.4252019 ]
 [-1.6597116  -5.79077511 -3.49558499 -5.13408408 -0.85032574 -6.9173662
  -2.11586901 -1.47594555]
 [-1.63965261 -5.60343005 -3.46225676 -5.44767245 -0.85588044 -6.72497141
  -2.0651697  -1.54096085]
 [-1.64650202 -5.66740153 -3.47363713 -5.34059349 -0.85398371 -6.79066719
  -2.08248166 -1.51876051]
 [-1.64356656 -5.63998518 -3.46875983 -5.38648447 -0.8547966  -6.76251185
  -2.07506225 -1.52827494]
 [-1.66362555 -5.82733025 -3.50208806 -5.0728961  -0.84924189 -6.95490665
  -2.12576156 -1.46325964]
 [-1.66558253 -5.84560781 -3.5053396  -5.04230212 -0.84869997 -6.97367687
  -2.13070783 -1.45691668]
 [-1.64258807 -5.6308464  -3.46713406 -5.40178146 -0.85506756 -6.75312674
  -2.07258911 -1.53144642]
 [-1.67243194 -5.9095793  -3.51671997 -4.93522316 -0.84680324 -7.03937266
  -2.14801979 -1.43471634]
 [-1.64063109 -5.61256883 -3.46388253 -5.43237545 -0.85560948 -6.73435652
  -2.06764284 -1.53778938]
 [-1.65335143 -5.73137302 -3.4850175  -5.23351454 -0.85208699 -6.85636297
  -2.09979362 -1.49656016]
 [-1.65286219 -5.72680363 -3.48420462 -5.24116303 -0.85222247 -6.85167042
  -2.09855705 -1.4981459 ]
 [-1.64209883 -5.62627701 -3.46632118 -5.40942996 -0.85520304 -6.74843419
  -2.07135254 -1.53303216]
 [-1.66998572 -5.88673234 -3.51265555 -4.97346565 -0.84748064 -7.01590988
  -2.14183695 -1.44264503]
 [-1.6641148  -5.83189964 -3.50290094 -5.06524761 -0.84910641 -6.95959921
  -2.12699813 -1.4616739 ]
 [-1.6719427  -5.90500991 -3.51590708 -4.94287166 -0.84693872 -7.0346801
  -2.14678323 -1.43630208]
 [-1.64992673 -5.69938728 -3.47932732 -5.28705401 -0.85303535 -6.82351508
  -2.09113764 -1.50766033]
 [-1.64307731 -5.63541579 -3.46794694 -5.39413297 -0.85493208 -6.7578193
  -2.07382568 -1.52986068]
 [-1.64454504 -5.64912397 -3.4703856  -5.37118748 -0.85452564 -6.77189697
  -2.07753539 -1.52510346]
 [-1.66656102 -5.8547466  -3.50696536 -5.02700512 -0.84842901 -6.98306199
  -2.13318097 -1.45374521]
 [-1.67145345 -5.90044052 -3.5150942  -4.95052015 -0.8470742  -7.02998755
  -2.14554666 -1.43788781]
 [-1.67341043 -5.91871808 -3.51834573 -4.91992617 -0.84653228 -7.04875777
  -2.15049293 -1.43154486]
 [-1.66705026 -5.85931599 -3.50777825 -5.01935663 -0.84829353 -6.98775454
  -2.13441754 -1.45215947]
 [-1.66020085 -5.7953445  -3.49639787 -5.12643558 -0.85019026 -6.92205876
  -2.11710558 -1.47435981]
 [-1.66313631 -5.82276086 -3.50127518 -5.0805446  -0.84937737 -6.95021409
  -2.12452499 -1.46484538]
 [-1.65139446 -5.71309545 -3.48176597 -5.26410852 -0.85262891 -6.83759275
  -2.09484735 -1.50290312]
 [-1.67683514 -5.95070383 -3.52403592 -4.86638669 -0.84558392 -7.08160566
  -2.15914891 -1.42044468]
 [-1.65384068 -5.73594241 -3.48583039 -5.22586604 -0.85195151 -6.86105553
  -2.10103019 -1.49497442]
 [-1.67732438 -5.95527322 -3.5248488  -4.85873819 -0.84544844 -7.08629822
  -2.16038548 -1.41885895]
 [-1.65481916 -5.74508119 -3.48745615 -5.21056905 -0.85168054 -6.87044064
  -2.10350333 -1.49180294]
 [-1.66264707 -5.81819146 -3.50046229 -5.0881931  -0.84951285 -6.94552154
  -2.12328842 -1.46643112]
 [-1.65090521 -5.70852606 -3.48095308 -5.27175702 -0.85276439 -6.83290019
  -2.09361078 -1.50448886]
 [-1.66215782 -5.81362207 -3.49964941 -5.09584159 -0.84964833 -6.94082898
  -2.12205185 -1.46801686]
 [-1.67292119 -5.91414869 -3.51753285 -4.92757466 -0.84666776 -7.04406521
  -2.14925636 -1.4331306 ]
 [-1.67389967 -5.92328747 -3.51915862 -4.91227767 -0.8463968  -7.05345033
  -2.1517295  -1.42995912]
 [-1.648459   -5.6856791  -3.47688867 -5.3099995  -0.85344179 -6.80943741
  -2.08742793 -1.51241755]
 [-1.66851799 -5.87302416 -3.5102169  -4.99641114 -0.84788709 -7.00183221
  -2.13812725 -1.44740225]
 [-1.64601278 -5.66283214 -3.47282425 -5.34824199 -0.8541192  -6.78597463
  -2.08124509 -1.52034625]
 [-1.64796975 -5.68110971 -3.47607578 -5.317648   -0.85357727 -6.80474486
  -2.08619137 -1.51400329]
 [-1.65530841 -5.74965059 -3.48826904 -5.20292055 -0.85154506 -6.8751332
  -2.10473989 -1.4902172 ]
 [-1.66166858 -5.80905268 -3.49883653 -5.10349009 -0.84978382 -6.93613643
  -2.12081529 -1.4696026 ]
 [-1.67438892 -5.92785687 -3.5199715  -4.90462917 -0.84626132 -7.05814288
  -2.15296607 -1.42837338]
 [-1.63916336 -5.59886066 -3.46144387 -5.45532094 -0.85601592 -6.72027885
  -2.06393313 -1.54254659]
 [-1.66802875 -5.86845477 -3.50940401 -5.00405963 -0.84802257 -6.99713965
  -2.13689068 -1.44898799]
 [-1.67634589 -5.94613443 -3.52322304 -4.87403519 -0.8457194  -7.07691311
  -2.15791234 -1.42203042]]
Decision Function (Downcase) [[-1.6440558  -5.64455458 -3.46957271 -5.37883597 -0.85466112 -6.76720441
  -2.07629882 -1.5266892 ]
 [-1.64112034 -5.61713822 -3.46469541 -5.42472695 -0.855474   -6.73904907
  -2.06887941 -1.53620364]
 [-1.65824387 -5.77706694 -3.49314634 -5.15702957 -0.85073218 -6.90328853
  -2.11215931 -1.48070277]
 [-1.65579765 -5.75421998 -3.48908192 -5.19527205 -0.85140958 -6.87982575
  -2.10597646 -1.48863146]
 [-1.66509328 -5.84103842 -3.50452671 -5.04995061 -0.84883545 -6.96898432
  -2.12947127 -1.45850242]
 [-1.64943748 -5.69481788 -3.47851443 -5.29470251 -0.85317083 -6.81882253
  -2.08990107 -1.50924607]
 [-1.66117933 -5.80448329 -3.49802364 -5.11113859 -0.8499193  -6.93144387
  -2.11957872 -1.47118833]
 [-1.66900724 -5.87759356 -3.51102978 -4.98876264 -0.84775161 -7.00652477
  -2.13936381 -1.44581651]
 [-1.6518837  -5.71766484 -3.48257885 -5.25646003 -0.85249343 -6.84228531
  -2.09608391 -1.50131738]
 [-1.64699126 -5.67197093 -3.47445001 -5.33294499 -0.85384823 -6.79535975
  -2.08371823 -1.51717477]
 [-1.64552353 -5.65826275 -3.47201136 -5.35589048 -0.85425468 -6.78128208
  -2.08000852 -1.52193198]
 [-1.65041597 -5.70395667 -3.4801402  -5.27940552 -0.85289987 -6.82820764
  -2.09237421 -1.50607459]
 [-1.64014185 -5.60799944 -3.46306964 -5.44002395 -0.85574496 -6.72966396
  -2.06640627 -1.53937511]
 [-1.66069009 -5.7999139  -3.49721076 -5.11878708 -0.85005478 -6.92675131
  -2.11834215 -1.47277407]
 [-1.67047497 -5.89130173 -3.51346843 -4.96581715 -0.84734516 -7.02060243
  -2.14307352 -1.44105929]
 [-1.65237295 -5.72223424 -3.48339173 -5.24881153 -0.85235795 -6.84697786
  -2.09732048 -1.49973164]
 [-1.64160958 -5.62170762 -3.46550829 -5.41707846 -0.85533852 -6.74374163
  -2.07011597 -1.5346179 ]
 [-1.6675395  -5.86388538 -3.50859113 -5.01170813 -0.84815805 -6.9924471
  -2.13565411 -1.45057373]
 [-1.64503429 -5.65369336 -3.47119848 -5.36353898 -0.85439016 -6.77658952
  -2.07877195 -1.52351772]
 [-1.65726538 -5.76792815 -3.49152057 -5.17232656 -0.85100314 -6.89390342
  -2.10968617 -1.48387425]
 [-1.6562869  -5.75878937 -3.4898948  -5.18762355 -0.8512741  -6.88451831
  -2.10721303 -1.48704573]
 [-1.66460404 -5.83646903 -3.50371383 -5.05759911 -0.84897093 -6.96429176
  -2.1282347  -1.46008816]
 [-1.65922236 -5.78620572 -3.49477211 -5.14173257 -0.85046122 -6.91267365
  -2.11463244 -1.47753129]
 [-1.65775463 -5.77249755 -3.49233346 -5.16467806 -0.85086766 -6.89859598
  -2.11092274 -1.48228851]
 [-1.67096421 -5.89587112 -3.51428132 -4.95816865 -0.84720968 -7.02529499
  -2.14431009 -1.43947355]
 [-1.66607177 -5.85017721 -3.50615248 -5.03465362 -0.84856449 -6.97836943
  -2.1319444  -1.45533094]
 [-1.64894824 -5.69024849 -3.47770155 -5.30235101 -0.85330631 -6.81412997
  -2.0886645  -1.51083181]
 [-1.66949648 -5.88216295 -3.51184266 -4.98111414 -0.84761613 -7.01121732
  -2.14060038 -1.44423077]
 [-1.64748051 -5.67654032 -3.4752629  -5.3252965  -0.85371275 -6.8000523
  -2.0849548  -1.51558903]
 [-1.65873312 -5.78163633 -3.49395922 -5.14938107 -0.8505967  -6.90798109
  -2.11339587 -1.47911703]
 [-1.65677614 -5.76335876 -3.49070769 -5.17997506 -0.85113862 -6.88921087
  -2.1084496  -1.48545999]
 [-1.67487816 -5.93242626 -3.52078439 -4.89698068 -0.84612584 -7.06283544
  -2.15420264 -1.42678764]
 [-1.67585665 -5.94156504 -3.52241015 -4.88168368 -0.84585488 -7.07222055
  -2.15667577 -1.42361616]
 [-1.65432992 -5.7405118  -3.48664327 -5.21821754 -0.85181602 -6.86574809
  -2.10226676 -1.49338868]
 [-1.67536741 -5.93699565 -3.52159727 -4.88933218 -0.84599036 -7.06752799
  -2.15543921 -1.4252019 ]
 [-1.6597116  -5.79077511 -3.49558499 -5.13408408 -0.85032574 -6.9173662
  -2.11586901 -1.47594555]
 [-1.63965261 -5.60343005 -3.46225676 -5.44767245 -0.85588044 -6.72497141
  -2.0651697  -1.54096085]
 [-1.64650202 -5.66740153 -3.47363713 -5.34059349 -0.85398371 -6.79066719
  -2.08248166 -1.51876051]
 [-1.64356656 -5.63998518 -3.46875983 -5.38648447 -0.8547966  -6.76251185
  -2.07506225 -1.52827494]
 [-1.66362555 -5.82733025 -3.50208806 -5.0728961  -0.84924189 -6.95490665
  -2.12576156 -1.46325964]
 [-1.66558253 -5.84560781 -3.5053396  -5.04230212 -0.84869997 -6.97367687
  -2.13070783 -1.45691668]
 [-1.64258807 -5.6308464  -3.46713406 -5.40178146 -0.85506756 -6.75312674
  -2.07258911 -1.53144642]
 [-1.67243194 -5.9095793  -3.51671997 -4.93522316 -0.84680324 -7.03937266
  -2.14801979 -1.43471634]
 [-1.64063109 -5.61256883 -3.46388253 -5.43237545 -0.85560948 -6.73435652
  -2.06764284 -1.53778938]
 [-1.65335143 -5.73137302 -3.4850175  -5.23351454 -0.85208699 -6.85636297
  -2.09979362 -1.49656016]
 [-1.65286219 -5.72680363 -3.48420462 -5.24116303 -0.85222247 -6.85167042
  -2.09855705 -1.4981459 ]
 [-1.64209883 -5.62627701 -3.46632118 -5.40942996 -0.85520304 -6.74843419
  -2.07135254 -1.53303216]
 [-1.66998572 -5.88673234 -3.51265555 -4.97346565 -0.84748064 -7.01590988
  -2.14183695 -1.44264503]
 [-1.6641148  -5.83189964 -3.50290094 -5.06524761 -0.84910641 -6.95959921
  -2.12699813 -1.4616739 ]
 [-1.6719427  -5.90500991 -3.51590708 -4.94287166 -0.84693872 -7.0346801
  -2.14678323 -1.43630208]
 [-1.64992673 -5.69938728 -3.47932732 -5.28705401 -0.85303535 -6.82351508
  -2.09113764 -1.50766033]
 [-1.64307731 -5.63541579 -3.46794694 -5.39413297 -0.85493208 -6.7578193
  -2.07382568 -1.52986068]
 [-1.64454504 -5.64912397 -3.4703856  -5.37118748 -0.85452564 -6.77189697
  -2.07753539 -1.52510346]
 [-1.66656102 -5.8547466  -3.50696536 -5.02700512 -0.84842901 -6.98306199
  -2.13318097 -1.45374521]
 [-1.67145345 -5.90044052 -3.5150942  -4.95052015 -0.8470742  -7.02998755
  -2.14554666 -1.43788781]
 [-1.67341043 -5.91871808 -3.51834573 -4.91992617 -0.84653228 -7.04875777
  -2.15049293 -1.43154486]
 [-1.66705026 -5.85931599 -3.50777825 -5.01935663 -0.84829353 -6.98775454
  -2.13441754 -1.45215947]
 [-1.66020085 -5.7953445  -3.49639787 -5.12643558 -0.85019026 -6.92205876
  -2.11710558 -1.47435981]
 [-1.66313631 -5.82276086 -3.50127518 -5.0805446  -0.84937737 -6.95021409
  -2.12452499 -1.46484538]
 [-1.65139446 -5.71309545 -3.48176597 -5.26410852 -0.85262891 -6.83759275
  -2.09484735 -1.50290312]
 [-1.67683514 -5.95070383 -3.52403592 -4.86638669 -0.84558392 -7.08160566
  -2.15914891 -1.42044468]
 [-1.65384068 -5.73594241 -3.48583039 -5.22586604 -0.85195151 -6.86105553
  -2.10103019 -1.49497442]
 [-1.67732438 -5.95527322 -3.5248488  -4.85873819 -0.84544844 -7.08629822
  -2.16038548 -1.41885895]
 [-1.65481916 -5.74508119 -3.48745615 -5.21056905 -0.85168054 -6.87044064
  -2.10350333 -1.49180294]
 [-1.66264707 -5.81819146 -3.50046229 -5.0881931  -0.84951285 -6.94552154
  -2.12328842 -1.46643112]
 [-1.65090521 -5.70852606 -3.48095308 -5.27175702 -0.85276439 -6.83290019
  -2.09361078 -1.50448886]
 [-1.66215782 -5.81362207 -3.49964941 -5.09584159 -0.84964833 -6.94082898
  -2.12205185 -1.46801686]
 [-1.67292119 -5.91414869 -3.51753285 -4.92757466 -0.84666776 -7.04406521
  -2.14925636 -1.4331306 ]
 [-1.67389967 -5.92328747 -3.51915862 -4.91227767 -0.8463968  -7.05345033
  -2.1517295  -1.42995912]
 [-1.648459   -5.6856791  -3.47688867 -5.3099995  -0.85344179 -6.80943741
  -2.08742793 -1.51241755]
 [-1.66851799 -5.87302416 -3.5102169  -4.99641114 -0.84788709 -7.00183221
  -2.13812725 -1.44740225]
 [-1.64601278 -5.66283214 -3.47282425 -5.34824199 -0.8541192  -6.78597463
  -2.08124509 -1.52034625]
 [-1.64796975 -5.68110971 -3.47607578 -5.317648   -0.85357727 -6.80474486
  -2.08619137 -1.51400329]
 [-1.65530841 -5.74965059 -3.48826904 -5.20292055 -0.85154506 -6.8751332
  -2.10473989 -1.4902172 ]
 [-1.66166858 -5.80905268 -3.49883653 -5.10349009 -0.84978382 -6.93613643
  -2.12081529 -1.4696026 ]
 [-1.67438892 -5.92785687 -3.5199715  -4.90462917 -0.84626132 -7.05814288
  -2.15296607 -1.42837338]
 [-1.63916336 -5.59886066 -3.46144387 -5.45532094 -0.85601592 -6.72027885
  -2.06393313 -1.54254659]
 [-1.66802875 -5.86845477 -3.50940401 -5.00405963 -0.84802257 -6.99713965
  -2.13689068 -1.44898799]
 [-1.67634589 -5.94613443 -3.52322304 -4.87403519 -0.8457194  -7.07691311
  -2.15791234 -1.42203042]]
#+end_example

# two draws w/o replacement of anchors and successors; compare
# permute
# re-draw same anchors with new successors, and then compare all four



# rewritten <2023-10-31 Tue>

5. [@5] <<bow>> One feature that might be helpful in distinguishing
   machine-generated texts is repetition from sentence to sentence.  Let's
   explore that hypothesis informally.  One way we can approach that
   problem is by comparing significant words (that is, non-stop words)
   between each sentence and its successor sentence in a corpus.  We'll use
   the Brown corpus, category news, for this problem.  Go ahead and
   downcase unless you want to explain why you don't think it's a good
   idea.

    For 700 randomly selected sentences in the Brown news corpus (the
   ``first anchor sentences''), and their immediately succeeding sentences
   (''first set of successors''), form bags of the significant words for
   each pair of sentences (so really, these are paired data) and then
   compute the intersection between each pair of bags (so, 700 intersecting
   bags of words).  This is the baseline comparison.

   Tabulate these and compare the results to another randomly drawn 700
   sentences (''second anchor sentences''), ensuring that there are no
   duplicates between the first anchor sentences and the second anchor
   sentences and their successors (sampling without replacement).  Permute
   all sentences in the corpus, then draw the same second anchor sentences
   and their new successors and repeat computing the intersecting bags of
   significant words.   So you'll have three intersecting bags of
   significant words:
      + first anchor and their successors (overall baseline)
      + second anchor and their successors, ensuring that intersection of
        first and second anchors is empty (control for second anchors BOWs)
      + second anchor and new successors after permutation (the experiment).


   Compare and discuss the results.  You may use methods from NLTK,
   scikit-learn, or both for this problem: just be sure your code block is
   complete and self-contained.

#+BEGIN_SRC python

import random
import nltk
from nltk.corpus import brown
from nltk.corpus import stopwords
from collections import Counter
from sklearn.utils import shuffle

nltk.download('brown')
nltk.download('stopwords')

# Load Brown corpus category news and remove the stop words
stop_words = set(stopwords.words("english"))
news_sentences = brown.sents(categories='news')

# Function to create bag of significant words
def significant_words(sentence):
    return set(word.lower() for word in sentence if word.isalpha() and word.lower() not in stop_words)

# First set of anchor sentences and successors
# Using random.seed to keep same random results when ran
random.seed(8)

first_anchors = random.sample(range(len(news_sentences) - 1), 700)
base_intersections = []
for i in first_anchors:
    anchor_bow = significant_words(news_sentences[i])
    successor_bow = significant_words(news_sentences[i + 1])
    base_intersections.append(len(anchor_bow.intersection(successor_bow)))

# Second set of anchor sentences and successors
second_anchor_pool = set(range(len(news_sentences) - 1)) - set(first_anchors)
second_anchors = random.sample(second_anchor_pool, 700)
control_intersections = []
for i in second_anchors:
    anchor_bow = significant_words(news_sentences[i])
    successor_bow = significant_words(news_sentences[i + 1])
    control_intersections.append(len(anchor_bow.intersection(successor_bow)))

# Second anchors with permuted successors
shuffle_sentences = shuffle(news_sentences, random_state=42)
exp_intersections = []
for i in second_anchors:
    anchor_bow = significant_words(shuffle_sentences[i])
    successor_bow = significant_words(shuffle_sentences[i + 1])
    exp_intersections.append(len(anchor_bow.intersection(successor_bow)))

# Displaying Results
print("(first anchor + successors):", Counter(base_intersections))
print("(second anchor + successors):", Counter(control_intersections))
print("(second anchor + new successors):", Counter(exp_intersections))


#+END_SRC


# added notes on not relying on default cluster nums and
# checking quality of drawn data and enclosing plots
#
# Kazic, 13.12.2023
   
6. [@6] <<ml>> Choose 500 random sentences from the Brown news corpus
   tagged with the universal tagset.  For each sentence, identify the nouns
   and then compile a list of the three tags immediately preceeding each
   noun (excluding chunks smaller than three). (Ensure all your chosen
   sentences have at least one noun that occurs fourth or later in the
   list.)  Then encode each chunk in the list as a ternary tuple, mapping
   the tags to integers, and save the list of mapped tuples to disk using
   either pickle or json, as you wish.  Go ahead and downcase unless you
   want to explain why you don't think it's a good idea.

   Now cluster your saved encodings using the [[http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift][MeanShift]] and
   [[https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering][AgglomerativeClustering]] algorithms from scikit-learn.  Do not rely on
   the default number of clusters for each method, but determine a good
   number of clusters for your data.  For MeanShift, take the remaining
   defaults; you may run it in parallel if you like and can.  Run
   AgglomerativeClustering thrice: first using "ward" for the linkage (it
   will only use a Euclidean metric); and then "average" for the linkage
   and "manhattan" and "cosine" for the metric.  Discuss the differences
   you find in the results of the different trials, referencing and
   enclosing your plots.

#+BEGIN_SRC python
import pickle
import nltk
from nltk.corpus import brown
from sklearn.preprocessing import LabelEncoder
import random

nltk.download('brown')
nltk.download('universal_tagset')

brown_sents = list(brown.tagged_sents(categories='news', tagset='universal'))

selected_sents = random.sample(brown_sents, 500)  

chunk_list = []

for sent in selected_sents:
    nouns_indices = [i for i, (_, tag) in enumerate(sent) if tag == 'NOUN']
    
    for noun_index in nouns_indices:
        if noun_index < 3:
            continue
        
        preceding_tags = [sent[noun_index - 3][1], sent[noun_index - 2][1], sent[noun_index - 1][1]]
        chunk_list.append(preceding_tags)

label_encoder = LabelEncoder()
all_tags = [tag for chunk in chunk_list for tag in chunk]
label_encoder.fit(all_tags)

encoded_chunks = []
for chunk in chunk_list:
    encoded_chunks.append(tuple(label_encoder.transform(chunk)))

with open('encoded_chunks.pkl', 'wb') as f:
    pickle.dump(encoded_chunks, f)

print(f"Encoded chunks saved to disk. Total chunks: {len(encoded_chunks)}")

#+END_SRC



* 
* DONE Grading Scale

This homework is worth 15 points. The grading scale is:


| fraction correctly reviewed and answered | points awarded |
|------------------------------------------+----------------|
| \(\ge 0.95\)                             |             15 |
| 0.90 -- 0.94                             |             14 |
| 0.85 -- 0.89                             |             13 |
| 0.80 -- 0.94                             |             12 |
| 0.75 -- 0.79                             |             11 |
| 0.70 -- 0.74                             |             10 |
| 0.65 -- 0.69                             |              9 |
| 0.60 -- 0.64                             |              8 |
| 0.55 -- 0.59                             |              7 |
| 0.50 -- 0.54                             |              6 |
| 0.45 -- 0.49                             |              5 |
| 0.40 -- 0.44                             |              4 |
| 0.35 -- 0.39                             |              3 |
| 0.30 -- 0.34                             |              2 |
| 0.25 -- 0.29                             |              1 |
| \(< 0.25\)                               |              0 |








* DONE Scoring


|     question | max pts | answer ok? |
|--------------+---------+------------|
|            1 |       1 |            |
|            2 |       1 |            |
|            3 |       2 |            |
|            4 |       2 |            |
|            5 |       2 |            |
|            6 |       7 |            |
|--------------+---------+------------|
|  total score |      15 |            |
|   percentage |         |            |
| total points |         |            |
#+TBLFM: @8$2=vsum(@I..@II)::@8$3=vsum(@I..@II)::@9$3=@-1/@-1$-1



